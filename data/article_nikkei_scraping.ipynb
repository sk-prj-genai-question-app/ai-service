{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0db7b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdc81d",
   "metadata": {},
   "source": [
    "#### 1. 초기 설정 및 라이브러리 임포트\n",
    "이 셀에서는 필요한 라이브러리를 임포트하고, 스크래핑에 필요한 초기 변수들을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728e8016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스크래핑 시작을 위한 초기 설정이 완료되었습니다.\n",
      "기본 URL 템플릿: https://business.nikkei.com/atcl/gen/19/00461/?TOC={page_num}\n",
      "초기 페이지 번호: 1\n",
      "초기 키워드: business\n",
      "저장될 디렉토리: c:\\sk\\mini_project3\\ai-service\\data\\article_data\n",
      "사용될 User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\n",
      "쿠키 파일 경로: nikkei_cookies.json\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json # JSON 파일 처리를 위해 추가\n",
    "\n",
    "# 스크래핑할 기본 URL 설정\n",
    "BASE_URL_TEMPLATE = \"https://business.nikkei.com/atcl/gen/19/00461/?TOC={page_num}\"\n",
    "# 초기 페이지 번호\n",
    "initial_page_num = 1\n",
    "# 초기 키워드 설정\n",
    "initial_keyword = \"business\"\n",
    "# 초기 ID 번호 (문자열 형식)\n",
    "initial_id_num = \"001\"\n",
    "\n",
    "# 저장할 디렉토리 설정: 코드가 위치한 루트 디렉토리의 'article_data' 폴더\n",
    "output_dir = os.path.join(os.getcwd(), \"article_data\")\n",
    "\n",
    "# 출력 디렉토리 생성 (없을 경우)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 봇으로 인식되지 않기 위한 User-Agent 헤더 설정\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',\n",
    "    'Referer': 'https://business.nikkei.com/' # Referer 추가: 스크래핑 시작점 도메인으로 설정\n",
    "}\n",
    "\n",
    "# 쿠키 파일 경로 (스크립트와 같은 디렉토리에 저장, 만료 시 다시 로그인해서 갱신 필요. ID와 비밀번호는 vesabeg992@datingso.com)\n",
    "COOKIES_FILE = 'nikkei_cookies.json'\n",
    "\n",
    "print(f\"스크래핑 시작을 위한 초기 설정이 완료되었습니다.\")\n",
    "print(f\"기본 URL 템플릿: {BASE_URL_TEMPLATE}\")\n",
    "print(f\"초기 페이지 번호: {initial_page_num}\")\n",
    "print(f\"초기 키워드: {initial_keyword}\")\n",
    "print(f\"저장될 디렉토리: {output_dir}\")\n",
    "print(f\"사용될 User-Agent: {headers['User-Agent']}\")\n",
    "print(f\"쿠키 파일 경로: {COOKIES_FILE}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c2ea2",
   "metadata": {},
   "source": [
    "#### 2. 페이지 URL에서 기사 링크 추출 함수\n",
    "이 셀에서는 주어진 페이지 URL에서 모든 기사 링크를 추출하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a5f329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# requests.Session() 객체를 전역으로 생성 (이전 셀에선 선언되지 않았으므로 이 위치에 추가)\n",
    "session = requests.Session()\n",
    "\n",
    "def get_article_links(page_url):\n",
    "    \"\"\"\n",
    "    주어진 URL에서 기사 링크를 추출합니다.\n",
    "    무료 기사 (class=\"p-articleList_item_date -titleLock -nbodd\")만 필터링합니다.\n",
    "    \"\"\"\n",
    "    print(f\"\\n페이지에서 기사 링크를 추출합니다: {page_url}\")\n",
    "    try:\n",
    "        # headers를 요청에 포함하고 session 객체 사용\n",
    "        response = session.get(page_url, headers=headers)\n",
    "        response.raise_for_status()  # HTTP 오류 발생 시 예외 발생\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"URL 접속 중 오류 발생: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # <div class=\"p-articleList_list\"> 컨테이너 찾기\n",
    "    article_list_container = soup.find('div', class_='p-articleList_list')\n",
    "    \n",
    "    if not article_list_container:\n",
    "        print(\"기사 목록 컨테이너를 찾을 수 없습니다.\")\n",
    "        return []\n",
    "    \n",
    "    # 모든 <a class=\"p-articleList_item_link\"> 태그 조회\n",
    "    links = article_list_container.find_all('a', class_='p-articleList_item_link')\n",
    "    \n",
    "    article_urls = []\n",
    "    for link in links:\n",
    "        # link (<a> 태그) 내부에 무료 기사를 나타내는 <p> 태그가 있는지 확인\n",
    "        # <p class=\"p-articleList_item_date -titleLock -nbodd\">\n",
    "        free_article_indicator = link.find('p', class_='p-articleList_item_date -titleLock -nbodd')\n",
    "        \n",
    "        if free_article_indicator: # 무료 기사 지표가 존재하는 경우에만 링크 추출\n",
    "            href = link.get('href')\n",
    "            if href:\n",
    "                # 상대 경로를 절대 경로로 변환 (필요한 경우)\n",
    "                if not href.startswith('http'):\n",
    "                    href = f\"https://business.nikkei.com{href}\" \n",
    "                article_urls.append(href)\n",
    "        else:\n",
    "            # 유료 기사 링크는 건너뜀 (선택 사항: 디버깅을 위해 이 부분에 print를 넣을 수도 있음)\n",
    "            # print(f\"  유료 기사 링크 건너뜀: {link.get('href')}\")\n",
    "            pass\n",
    "            \n",
    "    print(f\"총 {len(article_urls)}개의 무료 기사 링크를 찾았습니다.\")\n",
    "    return article_urls\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a33ea1",
   "metadata": {},
   "source": [
    "#### 3. 단일 기사 페이지 스크래핑 및 저장 함수\n",
    "이 셀에서는 각 기사 페이지로 이동하여 날짜, 본문 내용을 스크래핑하고 마크다운 파일로 저장하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e05360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def scrape_article_and_save(article_url, current_keyword, current_id_num):\n",
    "    \"\"\"\n",
    "    단일 기사 페이지를 스크래핑하고 마크다운 파일로 저장합니다.\n",
    "    \"\"\"\n",
    "    print(f\"\\n기사 스크래핑 중: {article_url}\")\n",
    "    try:\n",
    "        # headers를 요청에 포함하고 session 객체 사용\n",
    "        response = session.get(article_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"기사 페이지 접속 중 오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 기사 컨테이너 찾기\n",
    "    article_container = soup.find('div', class_='articleBody p-article_body') \n",
    "    if not article_container:\n",
    "        print(\"기사 컨테이너를 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    # 제목 스크래핑\n",
    "    title_tag = article_container.find('p', class_='p-article_header_title')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "    print(f\"  제목: {title}\\n\") # 줄바꿈 추가\n",
    "\n",
    "    # 작성 날짜 스크래핑\n",
    "    datetime_str = \"unknown_date\"\n",
    "    date_item = soup.find('li', class_='p-article_header_meta_item -date')\n",
    "    if date_item:\n",
    "        time_tag = date_item.find('time')\n",
    "        if time_tag and 'datetime' in time_tag.attrs:\n",
    "            datetime_str = time_tag['datetime']\n",
    "            print(f\"  작성 날짜: {datetime_str}\")\n",
    "        else:\n",
    "            print(\"  작성 날짜 (time 태그 또는 datetime 속성)를 찾을 수 없습니다.\")\n",
    "    else:\n",
    "        print(\"  작성 날짜 (li 태그)를 찾을 수 없습니다.\")\n",
    "\n",
    "    # 본문 내용 스크래핑\n",
    "    paragraphs = article_container.find_all('p')\n",
    "    article_text = []\n",
    "    for p in paragraphs:\n",
    "        cleaned_text = p.get_text(strip=True)\n",
    "        if cleaned_text: \n",
    "            article_text.append(cleaned_text)\n",
    "            \n",
    "    full_article_content = \"\\n\".join(article_text)\n",
    "    \n",
    "    # 파일 이름 생성\n",
    "    file_date = datetime_str\n",
    "    if re.match(r\"\\d{4}-\\d{2}-\\d{2}\", datetime_str): \n",
    "        file_date = datetime_str\n",
    "    elif \"T\" in datetime_str: \n",
    "        file_date = datetime_str.split('T')[0]\n",
    "    \n",
    "    filename = f\"article_nikkei_{current_keyword}_{file_date}_{current_id_num}.md\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    # 마크다운 파일 내용 작성\n",
    "    markdown_content = f\"### {title}\\n\\n\"\n",
    "    markdown_content += full_article_content\n",
    "\n",
    "    # 파일 저장\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_content)\n",
    "        print(f\"  기사 저장 완료: {filepath}\")\n",
    "        return True\n",
    "    except IOError as e:\n",
    "        print(f\"파일 저장 중 오류 발생: {e}\")\n",
    "        return False\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d59cf1",
   "metadata": {},
   "source": [
    "#### 4. 메인 스크래핑 루프\n",
    "이 셀에서는 전체 스크래핑 과정을 제어하는 메인 루프를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e81ba84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nikkei_cookies.json'에서 33개의 쿠키를 세션에 성공적으로 로드했습니다.\n",
      "쿠키 로드 성공! 로그인된 상태로 기사 스크래핑을 시작합니다.\n",
      "세션이 로그인 상태인 것으로 보입니다. 스크래핑을 진행합니다.\n",
      "\n",
      "--- 현재 페이지: https://business.nikkei.com/atcl/gen/19/00109/?TOC=10 (페이지 번호: 10) ---\n",
      "\n",
      "\n",
      "페이지에서 기사 링크를 추출합니다: https://business.nikkei.com/atcl/gen/19/00109/?TOC=10\n",
      "총 0개의 무료 기사 링크를 찾았습니다.\n",
      "더 이상 기사 링크를 찾을 수 없거나 페이지가 존재하지 않습니다. 스크래핑을 종료합니다.\n",
      "\n",
      "--- 모든 스크래핑 작업이 완료되었습니다. ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def load_cookies_into_session(session, cookie_file):\n",
    "    \"\"\"\n",
    "    JSON 형식의 쿠키 파일을 읽어 requests.Session 객체에 로드합니다.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(cookie_file):\n",
    "        print(f\"오류: 쿠키 파일 '{cookie_file}'을 찾을 수 없습니다.\")\n",
    "        print(\"브라우저에서 수동으로 로그인 후 'EditThisCookie' 등의 확장 프로그램으로 쿠키를 JSON으로 내보내고 해당 파일을 스크립트와 같은 위치에 저장해주세요.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        with open(cookie_file, 'r', encoding='utf-8') as f:\n",
    "            cookies_data = json.load(f)\n",
    "        \n",
    "        for cookie in cookies_data:\n",
    "            # domain을 requests.cookies.RequestsCookieJar에 맞는 형식으로 처리\n",
    "            # .으로 시작하는 경우 제거 (requests는 자동으로 처리하지만 명시적으로 제거하여 혹시 모를 문제 방지)\n",
    "            domain = cookie.get('domain', '').lstrip('.') \n",
    "            \n",
    "            # expirationDate가 존재하고 유효한 경우만 expires 설정\n",
    "            expires_at = cookie.get('expirationDate')\n",
    "            if expires_at:\n",
    "                try:\n",
    "                    # JavaScript timestamp는 초 단위, Python time.time()도 초 단위\n",
    "                    expires_at = int(expires_at)\n",
    "                except (ValueError, TypeError):\n",
    "                    expires_at = None # 유효하지 않은 값은 None으로 처리\n",
    "\n",
    "            session.cookies.set(\n",
    "                cookie['name'],\n",
    "                cookie['value'],\n",
    "                domain=domain,\n",
    "                path=cookie.get('path', '/'),\n",
    "                expires=expires_at, # expires에 None 또는 유효한 timestamp 전달\n",
    "                secure=cookie.get('secure', False),\n",
    "                rest={'HttpOnly': cookie.get('httpOnly', False)}\n",
    "            )\n",
    "        print(f\"'{cookie_file}'에서 {len(cookies_data)}개의 쿠키를 세션에 성공적으로 로드했습니다.\")\n",
    "        return True\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"오류: 쿠키 파일 '{cookie_file}'이 유효한 JSON 형식이 아닙니다: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"쿠키 로드 중 알 수 없는 오류 발생: {e}\")\n",
    "        return False\n",
    "\n",
    "# 메인 스크래핑 루프\n",
    "current_page_num = initial_page_num\n",
    "current_keyword = initial_keyword\n",
    "current_id_int = int(initial_id_num) \n",
    "\n",
    "# 최대 스크래핑 페이지 수 설정 (선택 사항)\n",
    "max_pages_to_scrape = 20\n",
    "\n",
    "# 쿠키 로드 시도\n",
    "if not load_cookies_into_session(session, COOKIES_FILE):\n",
    "    print(\"쿠키 로드에 실패하여 스크래핑을 시작할 수 없습니다. 쿠키 파일을 확인해주세요.\")\n",
    "else:\n",
    "    print(\"쿠키 로드 성공! 로그인된 상태로 기사 스크래핑을 시작합니다.\")\n",
    "    \n",
    "    # 로그인 상태 확인 (선택 사항: 실제로 로그인되었는지 확인하기 위한 요청)\n",
    "    try:\n",
    "        # business.nikkei.com의 메인 페이지 또는 로그인 후에만 접근 가능한 페이지로 테스트\n",
    "        test_response = session.get(\"https://business.nikkei.com/\", headers=headers, allow_redirects=True)\n",
    "        test_response.raise_for_status()\n",
    "        \n",
    "        # 'ログイン' (로그인) 텍스트가 페이지에 있고, 'ログアウト' (로그아웃) 텍스트가 없으면 로그인 안된 걸로 간주\n",
    "        # (이 부분은 웹사이트의 실제 로그인/로그아웃 텍스트에 따라 다를 수 있습니다.)\n",
    "        if \"ログイン\" in test_response.text and \"ログアウト\" not in test_response.text:\n",
    "             print(\"경고: 쿠키 로드 후에도 로그인 상태가 아닌 것으로 보입니다. 쿠키가 만료되었거나 올바르지 않을 수 있습니다.\")\n",
    "             print(\"스크래핑이 예상대로 작동하지 않을 수 있습니다.\")\n",
    "        else:\n",
    "            print(\"세션이 로그인 상태인 것으로 보입니다. 스크래핑을 진행합니다.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"로그인 상태 확인 중 오류 발생: {e}. 스크래핑을 진행합니다.\")\n",
    "\n",
    "    while True:\n",
    "        page_url = BASE_URL_TEMPLATE.format(page_num=current_page_num)\n",
    "        print(f\"\\n--- 현재 페이지: {page_url} (페이지 번호: {current_page_num}) ---\\n\") # 줄바꿈 추가\n",
    "\n",
    "        article_links = get_article_links(page_url)\n",
    "\n",
    "        if not article_links:\n",
    "            print(f\"더 이상 기사 링크를 찾을 수 없거나 페이지가 존재하지 않습니다. 스크래핑을 종료합니다.\")\n",
    "            break\n",
    "\n",
    "        for link in article_links:\n",
    "            formatted_id_num = f\"{current_id_int:03d}\"\n",
    "            \n",
    "            success = scrape_article_and_save(link, current_keyword, formatted_id_num)\n",
    "            if success:\n",
    "                current_id_int += 1 \n",
    "            else:\n",
    "                print(f\"경고: {link} 스크래핑에 실패했습니다. 다음 기사로 넘어갑니다.\")\n",
    "            \n",
    "            time.sleep(1) \n",
    "\n",
    "        print(f\"\\n페이지 {current_page_num}의 모든 기사 스크래핑을 완료했습니다.\")\n",
    "        \n",
    "        current_page_num += 1\n",
    "        \n",
    "        if 'max_pages_to_scrape' in locals() and current_page_num > initial_page_num + max_pages_to_scrape -1 :\n",
    "            print(f\"설정된 최대 스크래핑 페이지 수({max_pages_to_scrape} 페이지)에 도달했습니다. 스크래핑을 종료합니다.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "print(\"\\n--- 모든 스크래핑 작업이 완료되었습니다. ---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-M3_YSb4G-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
