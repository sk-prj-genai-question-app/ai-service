###

（第 1 回から読む）（第 2 回から読む）
OpenAI が 2022 年 11 月 30 日に「リサーチプレビュー」として試験的に ChatGPT を一般公開したとき、「ChatGPT は、一見もっともらしく見えても、内容が不正確か、あるいはまったくでたらめな答えを出力することがあります」と公式ブログで警告した。
それからたった 5 日で、100 万人が ChatGPT を試すために登録をした。そして彼らが使用感を語るにつれて、ChatGPT がもたらす「ハルシネーション（幻覚）」（誤りやねつ造、あるいはアルゴリズムの異常による奇妙な答えがたびたび出力されること）が、ソーシャルメディアやニュースで注目を浴び、この新奇なチャットボットの第一印象を決めるのに一役買った。
そのため、これから取りあげる例の一部は、もはや「古い」情報に思えるかもしれないが、そこは大目に見てもらいたい。
これらの例を読んで、少し古い情報だと思ったとしたら、その感覚は正しい。ここで話題になっている「それ」は、ChatGPT ではなく Wikipedia を指していて、例はすべて、2000 年代中盤の記事から引用したものだからだ。
もう少し考察を広げてみよう。1990 年代を通じて、「それ」はまさにインターネットそのものであり、まだまだ力をもっていたオールドメディアの門番たちにとっては、ヒラリー・クリントンが養子であるエイリアンの赤ん坊と一緒に写った写真が表紙を飾るウィークリー・ワールド・ニューズ誌よりも信用できないものだった。ワシントン D.C.の静かな夜に耳を澄ませば、ウェブニュースのパイオニアであるマット・ドラッジが、自分のレポートが正しい確率は 8 割ぐらいだと放言して、記者クラブのレポーターたちをいら立たせたときに巻き起こった抗議の声の反響がまだ聞こえてくるようだ。
とはいえ、いまこんな話をもち出したのは、大規模言語モデル（LLM）のパフォーマンスについて、自己満足の擁護を繰り広げるためではない。私自身、GPT-4 が私のポッドキャスト「マスター・オブ・スケール」に関して、でたらめな内容を出力したり、ソースを求めたら偽のリンク集を示したりするのを目の当たりにしている。さらに、文章のなかで自信たっぷりにある「事実」を主張しておきながら、その 2 つあとの文章で矛盾する情報を（同じく自信たっぷりに）披露して、なんの断りもなくその「事実」を否定するのを見たこともある。
だから、LLM をできるかぎり精度の高い、信頼できるものにするために、私たちは最大限の努力を払うべきだと思うし、実際にそうしているところだ。
だが、このテクノロジーを追求するにあたっては、いま世間で言われているような懸念（新しいテクノロジーの問題点や予測不可能な点が、社会で引き起こすとされ危険も含めて）がけっして目新しいものではないことを念頭に置いておく必要がある。
人間は昔から、これまでの技術や権力関係、暗黙の了解や明示された価値観から自分たちがつくりあげてきた秩序が、無知や誤った認識によって覆されることを恐れてきた。その一例として、紀元前 370 年ごろのソクラテスを取りあげてみよう。
人間の知性がつねに崩壊の瀬戸際にあるという危機感をもつのは、別におかしなことではない。多くの調査や研究が示すとおり、人間が把握している事実などごくわずかだからだ。だが、この状況で、本当に強力な幻覚生成装置が必要だと言えるのだろうか？
では、ここで GPT-4 の力を借りて、偉大な SF 作家である H. G. ウェルズに思考実験をしてもらおう。1938 年にウェルズは、「世界の頭脳」と名付けた全世界規模の知的装置の構想を提唱した。
H. GPT. ウェルズよ、よくぞ言ってくれた！
井上大剛＝翻訳
［日経 BOOK プラス 2023 年 7 月 3 日付の記事を転載］
