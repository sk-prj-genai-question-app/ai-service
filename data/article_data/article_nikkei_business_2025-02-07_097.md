###

AI による音声クローン技術は、人間の声を模倣し、任意のテキストを自然な音声に変換する能力を持つ。
具体的には、まず、対象となる人物の音声データを収集し、そのピッチ、音色、アクセントなどの特徴を抽出する。次に、機械学習アルゴリズムを用いて、これらの音声特徴を再現するモデルを作成する。最後に、このモデルを使用して任意のテキストを自然な音声に変換する。
5、6 年前まで、人の声を複製するには 15 ～ 30 分程度の音声データを必要とし、さらに音声の学習と合成などの処理に数時間から数日かかっていた。しかし、最近の技術進歩により、現在ではわずか数秒の音声サンプルからほぼリアルタイムに高品質な音声クローンの生成が可能になっている。これは、Facebook ページや TikTok に投稿された 30 秒程度の音声から十分な品質の音声クローンを作成できることを意味する。
さらに、技術の民主化も進んでいる。あるスタートアップ企業は、音声クローンツールを無料～月額 330 ドル程度で提供している。これにより、高度な技術知識を持たない個人でも容易に音声クローン技術を利用できるようになっている。
音声クローン技術は、私たちの生活やさまざまな産業分野に革新的な変化をもたらしている。医療分野では、声を失った患者のために音声を再現し、コミュニケーションを支援している。教育分野では、言語学習アプリケーションに活用され、学習者がさまざまなアクセントや発音を聞き、練習することを可能にしている。
エンターテインメント業界では、2022 年に 36 年ぶりに続編が公開された映画「トップガン　マーヴェリック」でこの技術が活用され、話題を呼んだ。トム・クルーズ演じる主人公マーヴェリックのかつてのライバルであるアイスマンを演じたバル・キルマーの音声である。
バル・キルマーは 2014 年に咽頭がんの手術を受け、声が出せなくなっており、本映画の中でも咽頭がんを患っている設定に変更された。そのため、劇中で肉声を発したのは二言だけだったが、これは AI が生成したものだった。
これらの応用例は、音声クローン技術が社会に多大な恩恵をもたらす可能性を示している。しかし、その一方で悪用されれば深刻な問題を引き起こす恐れがある。特に懸念されているのが、いわゆる「オレオレ詐欺（特殊詐欺）」の進化だ。
オレオレ詐欺は、主に高齢者をターゲットに詐欺師が電話で親族や知人を装って金銭を騙（だま）し取る特殊詐欺である。従来、この手法は詐欺師の話術や声に依存していたが、AI による音声クローン技術により、オレオレ詐欺は新たな段階に進化した。
まず、AI によって再現された音声クローンは本人の声に酷似しており、被害者を騙す確率が格段に高くなった。また、ソーシャルメディアなどから収集した個人情報を基に、よりリアルな状況を演出できるようになった。
さらに、詐欺行為の効率も格段に向上した。音声クローン技術を使って、音声メッセージを数百人に同時に送信することが容易になっているため、詐欺師は短時間で多くのターゲットに詐欺行為を仕掛けられる。
また安価な AI ツールの登場により、詐欺の実行コストも大幅に低下している。これらの要因が相まって、音声クローンを利用したオレオレ詐欺は従来の手法よりもはるかに効率的、かつ検出が困難なものとなっている。
音声クローンを利用したオレオレ詐欺の被害は、すでに世界各地で報告されている。2023 年 3 月、米国アリゾナ州の女性が AI を用いて生成された娘の声で誘拐を装った電話を受けた。この詐欺では、詐欺師が被害者の娘の声を完璧に再現し、「助けて！」と叫ぶ音声を使って、母親に金銭を要求した。
被害者はその声が本物だと信じ込み、パニック状態で金銭を支払おうとしたが、後に詐欺であることが発覚した。このような詐欺手法は高い感情的圧力を利用するため、被害者が冷静に対応することが難しく、多くのケースで金銭を騙し取られてしまう。
カナダでも同様の手口が報告されている。2023 年にサスカチュワン州で、AI で生成された孫の声に祖母が騙され、3000 カナダドルを詐欺師に送金してしまう事件が発生した。このケースでは詐欺師が孫の声を生成し、緊急で金銭が必要だと訴える電話をかけた。祖母はその声を信じ、すぐに対応してしまった。
カナダ当局はさらに深刻な事例として、ある男性が音声クローンを使用して、わずか 3 日間で 8 人から 20 万ドルを詐取したことを報告している。これは AI 技術を用いた詐欺の効率性と規模の大きさを如実に示すもので、問題の深刻さを物語っている。
AI を用いた音声クローン技術は、金融機関などが導入している音声認証システムにも影響を与えている。2023 年、アメリカのあるジャーナリストが自らの声を AI で作成し、それを使って銀行の音声認証システムの突破を試みる実験が成功したと報告した。この実験では、ある AI スタートアップが無料で提供している音声生成サービスを使用し、銀行口座にログインし、口座残高の参照や取引履歴の確認ができたという。
これは赤の他人であっても、わずか数秒の音声サンプルを入手できれば、銀行口座にアクセスできることを意味する。詐欺師は SNS（交流サイト）や YouTube などに投稿された個人の音声を悪用し、ほぼ無料で使える音声生成サービスで音声クローンを作り出す。本人確認のために求められる生年月日も SNS から見つける。
このため、著名人や政治家、インフルエンサーが狙われることが多い。音声認証システムを導入している銀行が多いアメリカでは、この脅威が深刻であり、すでに多くの銀行が代替の認証方法を検討しているという。
2023 年にアメリカの消費者が FTC（連邦取引委員会）に報告した詐欺による損失額 100 億ドルのうち、27 億ドルがなりすまし詐欺によるものだという。これはオンラインショッピング詐欺や投資詐欺などを抑え、詐欺のカテゴリの中では一番多く、2022 年度から 1 億ドルも増加している。
最近では、より洗練された詐欺手法も報告されている。2023 年、香港のある多国籍企業の財務担当者が、AI を用いたディープフェイク技術によって作成された偽のビデオ会議に参加させられ、2560 万ドル（約 35 億円）もの資金を詐取される事件が発生した。
この事件では、犯罪者たちは企業の CFO（Chief Financial Officer）を含む複数の幹部のディープフェイクを作成し、リアルタイムのビデオ会議を偽装した。被害者は、画面上で見た人物たちが本物の同僚だと信じ込み、「秘密の取引」のために多額の送金を行ってしまったのである。
これはビデオ会議という通常は安全だと考えられているコミュニケーション手段でさえも、AI によって操作される可能性があることを示している。単純な音声の模倣から始まり、今では視覚的要素も含めた総合的な偽装が可能になっており、一般人がこれらの詐欺を見抜くことはますます困難になっている。
現行の法制度では、AI による詐欺行為に対する法的な枠組みは十分に整備されておらず、企業の責任追及が難しい状況にある。
他の基盤技術と同様に、AI 企業が提供する音声クローン生成技術がどのように使われるかを事前に制御することは困難だからである。それだけに、詐欺行為が発生した後の対応や技術の悪用を防ぐためのガイドラインの作成等が必要になる。
すでに一部の企業は、生成された音声クローンを追跡できるようにしたり、匿名のユーザーは音声クローンを生成できないようにするなどの制限を設けたり、あるいは AI が生成した音声の識別ツールを提供するなどの対応策を講じているが、どこまで実効性があるかはまだ分からない。
また、2023 年初めには、米国の上院委員会で銀行セクターを監督する委員長が大手銀行の CEO（最高経営責任者）に対して音声認証サービスの利用状況を説明するよう求める書簡を送付した。内容は、「顧客が音声認証をどのくらいの頻度で使用しているか」「音声認証の不備による侵害に銀行はどのように対応しているか」といったもので、規制当局も監視の目を強めつつある。
警察庁によると、日本国内の 2023 年のオレオレ詐欺による被害額は 133 億円に達し、前年の 129 億円を上回っている。今後、日本でも音声クローンなどの生成 AI を活用した詐欺が広がり、被害が拡大する恐れがある。そのため、早急に技術開発者や利用者、そして規制当局が連携して悪用防止策を講じる必要がある。
ただし、単にこの技術を脅威とみなすだけではなく、社会に多大な恩恵をもたらす可能性があることも忘れてはならない。革新的な技術が進展する中で、悪用を防ぐための枠組みを構築しつつ、ポジティブな応用を促進するといったバランスが求められる。
技術そのものを悪者にするのではなく、その利用方法や規制によって、われわれはその恩恵を享受しつつ、リスクを最小限に抑える道を選ぶ必要がある。
［日経 BOOK プラス 2025 年 1 月 22 日付の記事を転載］
