###

GPT-4（同種の大規模言語モデルを含む）は意識をもたないが、さまざまな文脈で適切な文章を生成する能力を急成長させてきた結果、いまや人間に似た知性を備えているようにも見えつつある。そのため、LLM（大規模言語モデル）を説明するときに“知識”や“理解”といった言葉を使うことは許容できるし、実用的でさえあると思う。
この場合、厳密に文字どおりの意味ではなく、1976 年刊行のリチャード・ドーキンスの著書『利己的な遺伝子』（紀伊國屋書店刊）のタイトルと同じようにとらえたい。“利己的”という言葉が示唆するような主体感や自己認識が遺伝子にあるわけではない。しかし、どうしても人間中心的な考えをしがちな私たちにとって、この比喩があると遺伝子の機能を理解しやすくなる。
同様に、GPT-4 に人間のような心があるわけではない。それでも、少しだけ擬人化してその“視点”を考えることに意味はあるだろう。なぜなら“視点”という言葉を使うことで、GPT-4 の動作が決まりきった予測可能なものではないことを伝えられるからだ。
その意味でいえば、GPT-4 は人間のようなものではないか。ミスをすることもある。“考え方”も変化する。気まぐれな部分も十分にある。GPT-4 がこうした性質を示し、主体性があるかのように感じさせる振る舞いをすることも多いので、ここで AI が意識をもっているような表現を比喩的に使っていく。ここから先、“ ”といった引用符は取ってしまおう。
それでも読者のみなさんには、GPT-4 が意識のある存在ではないことを、そのすばらしい人間の心のなかにとどめておいてほしい。それを認識しておくことこそ、GPT-4 をいつ、どこで、どのように使えば最も生産的かつ責任ある使い方ができるのかを理解する鍵だと思う。
本質的に、GPT-4 は言語の流れを予測している。インターネット上に公開されている膨大な量のテキストから、個々の意味単位（単語、フレーズ、文章の全体または一部）のあいだに存在する最も一般的な関係を認識するよう学習した LLM は、ユーザーのプロンプトに対して、文脈上適切で、わかりやすい表現で、事実に沿った返答をかなりの確率で生成できる。
しかし一方で、事実と異なる内容、明らかに意味をなさない発言、あるいは文章としては（ある意味）適切に見えるが、実際にはまるで根拠のないでたらめを返してくることもある。
いずれの場合も、すべて計算とプログラミングにすぎないのだ。LLM は、少なくともいまのところは、常識的な推論をしたり世界の仕組みを考えたりするための事実や原理までは学ばない。あなたが質問しても、やりとりの目的について LLM は何も理解していない。LLM は生成する回答について事実関係の確認や倫理的判断をしているわけではなく、入力されたプロンプトの単語の並びに対して回答すべき内容をアルゴリズムで推測しているだけだ。
さらに、LLM が学習に利用するコーポラ（言語研究に使われるテキストのデータベース）は通常、バイアスや有害な内容を含む可能性のある公共のウェブソースをもとにしているため、人種差別、性差別、脅迫などの不快なコンテンツを生み出す可能性もある。
その点、開発者の操作によって LLM を特定の目的に適合させることはできる。たとえば OpenAI は、GPT-4 やほかの自社 LLM が生成できる内容を意図的に制限し、有害で、非倫理的で、安全でない文章を生成する能力を低くしている（たとえユーザーがそのような回答を望む場合でも）。
GPT-4 は前のモデルよりも高度な意識をもっているわけではない。予測する能力が高くなっただけだ。あらためて強調するが、GPT-4 は驚くほど見事な認知能力のシミュレーションをすることが多いが、あくまでそれはシミュレーションでしかない。GPT-4 は、映画『禁断の惑星』に登場するロボットの「ロビー」や『スタートレック』のアンドロイド「データ」のような、自意識や感覚を備えた主体ではないのだから。
それでも、文脈を理解する人間のような認識能力をシミュレートできること自体がすごい点も、もう一度強調しておきたい。なぜ私がそう思うのか。その理由は、数々の賞を受けた SF 作家のテッド・チャンがニューヨーカー誌に最近発表した評論が代弁してくれる。
ChatGPT はウェブ上のすべての文章をぼやけた JPEG 画像にしたものだと考えてみよう」とチャンは書いている。「JPEG に画像情報がたくさん埋め込まれているのと同様に、ChatGPT はウェブ上の情報をたくさん保持している。しかし、そこに正確なビット列を読み取ろうとしても見つかりはしない。得られるのはおよその値だけである」
チャンの考えでは、ChatGPT（GPT-4 など類似の LLM も含まれるだろう）が自身を構成する情報を正確に表現しないからこそ、それが情報合成能力にもハルシネーション（幻覚）やその他のエラーにもつながっている。「ウェブ上の全テキストを集めた JPEG」として一度にすべての情報にアクセスできるので、これまでになかった方法での情報合成が可能になる。そのため、ある事柄に関する知識とほかの事柄に関する知識を融合させ、説得力ある文章を新たに生み出せるのだ。
これについてチャンは、乾燥機の中で靴下が片方なくなる現象とアメリカ合衆国憲法を例にあげて説明している。ChatGPT はいずれも知っているので、その知識を使って新しいものをつくり上げられる。次のように、憲法ふうの文章で前者を語ることもできるのだ。
「人間の活動において、清潔さと秩序を維持するためには自身の衣服と他者の衣服を区別することが必要な場合があり……」確かに悪くない。ただし、ChatGPT はウェブの全体像を大まかに映す存在にすぎないので、（事実関係の正確さに難があるその本質に加えて）創造的な力はかなり限定的だ、とチャンは主張する。真に新しいものを生み出すことはできず、「既存の情報を再パッケージ化する」だけなのだと。
チャンのエッセイはじつに刺激的だが、「ウェブをまとめた JPEG 画像」という比喩は LLM の合成能力を過小評価しているとも私は感じる。
まず、入手できる情報を集めて再パッケージ化するということ自体、創造的であろうとなかろうと、人間が実現したイノベーションのうち巨大な割合を占めると思う。
そしてさらに重要なのは、LLM が知識を系統立てて蓄積するというまったく新しい力を実際に兼ね備え、その力を使っているという点だ。ウェブ上にははかり知れないほど膨大な情報が存在し、その多くは何十億というページに散らばっている。たとえばこんな質問について考えてみよう。
NFL（ナショナルフットボールリーグ）で MVP を受賞したランニングバックのうち最も背が高いのは誰か。人口 100 万人以上の都市のうち、女性市長がいたことがあるのはどこか。最も高齢でジェームズ・ボンドを演じたのは誰か。
これらに答えるためのデータがすでにウェブ上にあることは間違いない。
NFL の MVP 受賞者全員を漏れなくリストアップしているサイトは複数あるし、NFL 選手の身体的数値を正確にまとめているサイトもある。それでも、あなたがまさに探している情報を誰かがわざわざまとめて公開していないかぎり、ほしい情報にたどりつくまでにはかなりの時間がかかりかねない。たとえその情報が Wikipedia や NFL.com など 1 つのウェブサイトにあったとしても、複数のリンクにまたがっている可能性は高い。その場合もやはり答えを得るまでには多くの時間を費やすことになる。
（第 2 回に続く）
長尾莉紗＝翻訳
［日経 BOOK プラス 2023 年 6 月 29 日付の記事を転載］
