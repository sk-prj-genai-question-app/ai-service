###

AI の軍事への応用分野は、情報収集・分析、意思決定支援、自律型システム、サイバー攻撃、ロジスティクスなど多岐にわたる。これまで、人間の意思決定や操作に頼っていた戦闘行動が、AI によって部分的または完全に自動化されることで、迅速かつ正確な判断が可能となる。
例えば、自律型ドローンや自動化された監視システムが AI を搭載し、リアルタイムで目標を特定し、攻撃を実行に移す。これは戦場における反応速度を飛躍的に向上させ、犠牲者の減少につながる可能性がある。
しかし、その一方で、AI の誤作動やアルゴリズムの偏りによる誤判断が深刻な被害を引き起こすリスクも無視できない。無人兵器が誤って市民を攻撃するような事態が発生すれば、国際法違反や人権侵害につながる可能性がある。また、AI を利用した戦争が拡大することで、国家間の緊張が高まり、AI 技術が軍事競争を激化させるリスクも存在する。
このようなリスクを防ぐため、AI 技術の利用に関する国際的な枠組みが必要とされており、各国が協力して技術の適切な使用を推進することが重要である。
実は AI の軍事利用はすでに現実のものとなっている。特に、イスラエルは長年、ガザ地区を新しい軍事技術の実験場として利用しており、AI 搭載の兵器システムもその一環である。
特に注目されたのは、2021 年 5 月に行われたイスラエルの「ガザ侵攻」で、これは「初の人工知能戦争」と呼ばれている。イスラエルメディア「＋ 972 マガジン」は、イスラエル国防軍が AI を搭載した「Lavender（ラベンダー）」や「Gospel（ゴスペル）」と呼ばれるターゲティングシステムを使用したと報じている。
これらのシステムでは、大量の監視データを分析し、イスラム組織ハマスの戦闘員と疑われる人物を割り出し、攻撃目標を設定するために使われているという。ラベンダーは個人を、ゴスペルはインフラ施設をターゲットとしており、リアルタイムで人やインフラなどの攻撃目標を自動的に生成できる。
しかし、これらのシステムは完璧ではない。ラベンダーはハマス関係者を特定する際の誤認率が 10％であったにもかかわらず、イスラエル国防軍はラベンダーが自動で作成した「殺害リスト」を、あたかも人間の決定であるかのように自動的に採用することを全面的に承認したという。
つまり、AI が自動で生成した攻撃対象のリストを人間が確認することなしにそのまま採用し、誤って武装組織とは無関係なガザ市民を攻撃した可能性があるということだ。もし、これが事実であれば、倫理的にも人道的にも許されることではない。
また、こうした AI による意思決定プロセスは、人間の判断を排除し、大規模な破壊と殺戮（さつりく）を「客観的なアルゴリズムの結果」として正当化する危険性もある。
AI の軍事利用に対する国際的な懸念は、2023 年 12 月に国連総会で採択された「自律型致死兵器システム（LAWS）への対応が急務」という決議によって強調された。この決議では、AI 兵器が国際法に従って使用されることを確認しつつも、これが新たな軍拡競争を引き起こす可能性について警鐘が鳴らされた。
国連決議には、日本や米国を含む 152 カ国が賛成したものの、ロシアやインドは反対し、中国やイスラエルは棄権した。特に、イスラエルはガザ地区での AI 兵器の実用化を進めていることから、この技術に対する規制に慎重な立場を取っている。
AI 兵器が「第 3 の軍事革命」とも称される中、こうした技術の制御が困難であることが、国際的な不安を煽（あお）っている。
2024 年 9 月に韓国ソウルで開催された「Responsible AI in the Military Domain（REAIM：軍事分野における責任ある AI）」サミットでは、AI 技術の軍事利用に対する国際的な規制強化が主要なテーマとなった。このサミットでは、「Blueprint for Action（行動のための青写真）」が採択され、AI 技術が人権や国際法を侵害しない形で使用されるためのガイドラインが示された。
特に、核兵器の使用に関する意思決定に人間の管理と関与を維持することの重要性が強調された。
しかし、この合意には法的拘束力がなく、日本、米国、英国、オーストラリアなど 60 カ国以上が署名したものの、中国やロシアを含む主要な軍事国は署名を見送った。これにより、規制の実効性が疑問視されており、特に署名を拒否した国々が独自に軍事 AI 技術の開発を加速させる可能性が懸念されている。
AI 技術は、その二重用途性（デュアルユース）により、平時の民間利用と戦時の軍事利用が容易に混在する。例えば、AI を搭載した自動運転技術やドローンは、物流や医療救援に貢献する一方で、戦場での兵器としても活用される。このため、AI 技術の平和利用を推進する枠組みが必要とされているが、現状では規制の実効性に大きな課題が残っている。
さらに、グーグルやアマゾンなどの民間企業も、軍事機関に技術提供を行っていることが問題視されている。イスラエルがガザ地区で使用している AI システムの一部には、これらの企業のクラウド技術が使用されており、軍事目的に加担しているとの指摘もある。
これに対し、2024 年 5 月にはグーグル傘下の DeepMind の従業員約 200 名が、同社の AI 技術がイスラエルや米国の軍事機関に提供されていることに対して抗議した。従業員は、AI 技術が戦争の手段として使われることは、グーグルの「AI 原則」に反すると主張し、軍事目的での AI 技術の使用を停止するよう求めた。
この抗議運動は、企業が AI 技術をどのように活用するかについて、従業員レベルでの関与が進んでいることを表している。また、技術者たちが自身の技術がどのように利用されるかに対して責任を感じていることも浮き彫りにしている。企業は、AI 技術を開発・提供する際に、その利用が倫理的であるかを慎重に見極めなければならない。
一方で、グーグルはこの従業員からの抗議に対して、契約はクラウドコンピューティングサービスに関するものであり、軍事目的の AI 技術の提供には直接関与していないと説明しているが、従業員たちの不安は払拭されていない。
AI の軍事利用を防ぐには、国際社会の協力や企業の倫理的責任が必要とされる。しかし、技術の進化と各国の安全保障上の懸念が交錯する中で、すべての国や企業が同じ方向に進むとは限らない。特に、中国やロシアのように軍事技術の開発を進める国々が合意に参加しないことで、依然として AI 軍拡競争が激化するリスクが存在する。
この問題の本質は、技術そのものではなく、その技術をいかに管理し、使用するかということである。AI の軍事利用が完全に排除されることは難しいかもしれないが、国際的な協力と技術の適切な運用によって、そのリスクを最小限に抑え、安全で平和な未来を目指すことが可能であると信じたい。
［日経 BOOK プラス 2025 年 1 月 21 日付の記事を転載］
