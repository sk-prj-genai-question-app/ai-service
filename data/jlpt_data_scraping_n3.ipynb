{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cdd023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36470c4a",
   "metadata": {},
   "source": [
    "#### 1. ì›¹ ìŠ¤í¬ë˜í•‘ í™˜ê²½ ì„¤ì • ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "ì´ ì…€ì—ì„œëŠ” í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•˜ê³ , ë´‡ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ì•Šê¸° ìœ„í•œ ê¸°ë³¸ í—¤ë” ì„¤ì • ë° ìŠ¤í¬ë˜í•‘ ê³¼ì •ì—ì„œ ì‚¬ìš©í•  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# ë´‡ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ì•Šê¸° ìœ„í•œ í—¤ë” ì„¤ì •\n",
    "# ì‹¤ì œ ë¸Œë¼ìš°ì € User-Agentì™€ Accept-Languageë¥¼ ì‚¬ìš©í•˜ë©´ ë´‡ ê°ì§€ íšŒí”¼ì— ë„ì›€ì´ ë©ë‹ˆë‹¤.\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "}\n",
    "\n",
    "BASE_URL_FORMAT = \"https://dethitiengnhat.com/en/jlpt/N{level_num}\"\n",
    "\n",
    "def format_url(level_num, year, month, type_url):\n",
    "    \"\"\"ì£¼ì–´ì§„ íŒŒë¼ë¯¸í„°ë¡œ URLì„ êµ¬ì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    base_url = BASE_URL_FORMAT.format(level_num=level_num)\n",
    "    return f\"{base_url}/{year}{month}/{type_url}\"\n",
    "\n",
    "def get_content_with_underline(element):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ Beautiful Soup íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ë˜, <u> íƒœê·¸ëŠ” ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "    ì´ í•¨ìˆ˜ëŠ” ì—˜ë¦¬ë¨¼íŠ¸ ë‚´ì˜ ëª¨ë“  ìì‹ ë…¸ë“œë¥¼ ìˆœíšŒí•˜ë©° í…ìŠ¤íŠ¸ì™€ <u> íƒœê·¸ë¥¼ ì¡°í•©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    if not element:\n",
    "        return \"\"\n",
    "\n",
    "    parts = []\n",
    "    for content in element.contents: # ìì‹ ë…¸ë“œë¥¼ ìˆœíšŒ\n",
    "        if content.name == 'u': # <u> íƒœê·¸ì¸ ê²½ìš°\n",
    "            parts.append(f\"<u>{content.get_text(strip=True)}</u>\")\n",
    "        elif isinstance(content, str): # ì¼ë°˜ í…ìŠ¤íŠ¸ ë…¸ë“œì¸ ê²½ìš°\n",
    "            parts.append(content.strip())\n",
    "        else: # ê·¸ ì™¸ ë‹¤ë¥¸ íƒœê·¸ì¸ ê²½ìš° (ì˜ˆ: <span>, <div> ë“±), ë‚´ë¶€ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
    "            # ì—¬ê¸°ì„œëŠ” <u> íƒœê·¸ ì™¸ì˜ ë‹¤ë¥¸ íƒœê·¸ëŠ” ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •í•©ë‹ˆë‹¤.\n",
    "            # ë§Œì•½ ë‹¤ë¥¸ íŠ¹ì • íƒœê·¸ë„ ìœ ì§€í•´ì•¼ í•œë‹¤ë©´ ì—¬ê¸°ì— ì¶”ê°€ ë¡œì§ì„ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "            parts.append(content.get_text(strip=True))\n",
    "            \n",
    "    # ë¹ˆ ë¬¸ìì—´ì„ í•„í„°ë§í•˜ê³  ê³µë°±ìœ¼ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "    return \" \".join(filter(None, parts)).strip()\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"íŒŒì¼ ì´ë¦„ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ìë¥¼ ì œê±°í•˜ê±°ë‚˜ ëŒ€ì²´í•©ë‹ˆë‹¤.\"\"\"\n",
    "    return \"\".join(c for c in filename if c.isalnum() or c in ('_', '-')).strip()\n",
    "\n",
    "def save_to_markdown(filename, content):\n",
    "    \"\"\"ì£¼ì–´ì§„ ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # íŒŒì¼ëª…ì— ë¶€ì ì ˆí•œ ë¬¸ìê°€ í¬í•¨ë  ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ì •ê·œí™”í•©ë‹ˆë‹¤.\n",
    "    clean_filename = sanitize_filename(filename) + \".md\"\n",
    "    \n",
    "    # 'jlpt_data' í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "    output_dir = \"jlpt_data\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(output_dir, clean_filename)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        print(f\"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {file_path}\")\n",
    "    except IOError as e:\n",
    "        print(f\"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ '{file_path}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39535a4",
   "metadata": {},
   "source": [
    "#### 2. ìŠ¤í¬ë˜í•‘ ë¡œì§ ì •ì˜\n",
    "ì´ ì…€ì—ì„œëŠ” ì£¼ì–´ì§„ URLì—ì„œ ë°ì´í„°ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ê³ , ì´ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” í•µì‹¬ ë¡œì§ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_content(url, current_type): # current_type ì¸ì ì¶”ê°€\n",
    "\n",
    "    print(f\"â³ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}\")\n",
    "        # ë°˜í™˜ ê°’ ë³€ê²½: V ë°ì´í„°, G ë°ì´í„°, ì²« big_item_content\n",
    "        return None, None, None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # ë§ˆí¬ë‹¤ìš´ ì¶œë ¥ì„ ì„¸ ê°€ì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬: V ìœ í˜•, G ìœ í˜•\n",
    "    markdown_output_V = []\n",
    "    markdown_output_G = []\n",
    "    markdown_output_R = []\n",
    "    \n",
    "    first_big_item_content = \"\" # íŒŒì¼ëª… ê²°ì •ì„ ìœ„í•´ ì²« big_item ë‚´ìš©ì„ ì €ì¥\n",
    "    \n",
    "    # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ë¦¬í‚¤ëŠ” ë³€ìˆ˜\n",
    "    current_markdown_list = None \n",
    "\n",
    "    # í˜ì´ì§€ ë‚´ì˜ ëª¨ë“  ê´€ë ¨ ì—˜ë¦¬ë¨¼íŠ¸ë¥¼ ë¬¸ì„œ ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    all_relevant_elements = soup.find_all('div', class_=['big_item', 'question_content', 'question_list', 'answer_1row', 'answer_2row'])\n",
    "\n",
    "    if not all_relevant_elements:\n",
    "        print(f\"âš ï¸ í˜ì´ì§€ì—ì„œ ìŠ¤í¬ë˜í•‘í•  ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}\")\n",
    "        return \"# ë‚´ìš© ì—†ìŒ\\n\", \"# ë‚´ìš© ì—†ìŒ\\n\", \"\"\n",
    "    \n",
    "    for i, element in enumerate(all_relevant_elements):\n",
    "        if 'big_item' in element.get('class', []):\n",
    "            item_text = get_content_with_underline(element)\n",
    "            \n",
    "            # ì²« big_item ë‚´ìš©ì€ í•­ìƒ ì €ì¥ (íŒŒì¼ëª… ê²°ì •ìš©)\n",
    "            if i == 0:\n",
    "                first_big_item_content = item_text\n",
    "\n",
    "            # current_typeì´ 3ì¼ ë•Œë§Œ 'G'/'R' ë¶„ë¦¬ ë¡œì§ ì ìš©\n",
    "            if current_type == 3:\n",
    "                if any(keyword in item_text for keyword in ['å•é¡Œ4', 'å•é¡Œ 4', 'å•é¡Œï¼”', 'å•é¡Œ5', 'å•é¡Œ 5', 'å•é¡Œï¼•', 'å•é¡Œ6', 'å•é¡Œï¼–', 'å•é¡Œ 6', 'å•é¡Œ7', 'å•é¡Œ 7', 'å•é¡Œï¼—']):\n",
    "                    current_markdown_list = markdown_output_R\n",
    "                    current_markdown_list.append(f\"# {item_text}\\n\")\n",
    "                else:\n",
    "                    current_markdown_list = markdown_output_G\n",
    "                    current_markdown_list.append(f\"# {item_text}\\n\")\n",
    "            else: # current_typeì´ 1ì¼ ê²½ìš° (V ìœ í˜•)\n",
    "                current_markdown_list = markdown_output_V\n",
    "                current_markdown_list.append(f\"# {item_text}\\n\")\n",
    "        \n",
    "        # current_markdown_listê°€ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ ë‚´ìš© ì¶”ê°€\n",
    "        elif current_markdown_list is not None:\n",
    "            if 'question_content' in element.get('class', []):\n",
    "                content_text = get_content_with_underline(element)\n",
    "                current_markdown_list.append(f\"### {content_text}\\n\")\n",
    "                \n",
    "            elif 'question_list' in element.get('class', []):\n",
    "                question_text = get_content_with_underline(element)\n",
    "                current_markdown_list.append(f\"## {question_text}\\n\")\n",
    "            \n",
    "            elif 'answer_1row' in element.get('class', []) or 'answer_2row' in element.get('class', []):\n",
    "                current_markdown_list.append(\"#### ç­”ãˆ\\n\") \n",
    "                \n",
    "                answers = element.find_all('div', class_='answers')\n",
    "                if not answers:\n",
    "                    answers = element.find_all('li', class_='answers')\n",
    "\n",
    "                for answer in answers:\n",
    "                    answer_text = get_content_with_underline(answer)\n",
    "                    current_markdown_list.append(f\"- {answer_text}\\n\")\n",
    "                current_markdown_list.append(\"\\n\") # ê° ë‹µë³€ ê·¸ë£¹ ì‚¬ì´ì— ê³µë°± ì¶”ê°€\n",
    "    \n",
    "    # ìµœì¢… ë°˜í™˜ ê°’ ë³€ê²½: V ìœ í˜• ë°ì´í„°, G ìœ í˜• ë°ì´í„°, R ìœ í˜• ë°ì´í„°, ì²« big_item_content\n",
    "    return \"\\n\".join(markdown_output_V), \"\\n\".join(markdown_output_G), \"\\n\".join(markdown_output_R), first_big_item_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf932ed",
   "metadata": {},
   "source": [
    "#### 3. ë©”ì¸ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ ë£¨í”„\n",
    "ì´ ì…€ì—ì„œëŠ” URL ì´ë™ ê·œì¹™ì— ë”°ë¼ ë°˜ë³µì ìœ¼ë¡œ ì›¹í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ê³ , ì§€ì •ëœ íŒŒì¼ëª… ê·œì¹™ì— ë”°ë¼ .md íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ë©”ì¸ ë£¨í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77468941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤í¬ë˜í•‘ íŒŒë¼ë¯¸í„° ì´ˆê¸° ì„¤ì •\n",
    "level_num = 3\n",
    "current_year = 2019\n",
    "end_year = 2010\n",
    "current_month = '12'\n",
    "current_type = 1 # 1 ë˜ëŠ” 3\n",
    "\n",
    "print(\"--- ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œì‘ ---\")\n",
    "\n",
    "while True:\n",
    "    # URL êµ¬ì„±\n",
    "    date_url = f\"{current_year}{current_month}\"\n",
    "    url = format_url(level_num, current_year, current_month, current_type)\n",
    "\n",
    "    print(f\"\\nğŸ”„ í˜„ì¬ URL: {url}\")\n",
    "    \n",
    "    # í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ë° ë°ì´í„° ë°˜í™˜ (ë°˜í™˜ ê°’ì´ 3ê°œë¡œ ë³€ê²½ë¨!)\n",
    "    scraped_markdown_V, scraped_markdown_G, scraped_markdown_R, first_big_item_content = scrape_page_content(url, current_type) # current_type ì¸ì ì „ë‹¬\n",
    "    \n",
    "    if scraped_markdown_V is None: # ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ\n",
    "        print(f\"â— {url} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨. ë‹¤ìŒ URLë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.\")\n",
    "        # ì‹¤íŒ¨ ì²˜ë¦¬ ë¡œì§ (ì´ì „ê³¼ ë™ì¼)\n",
    "        if current_type == 1:\n",
    "            current_type = 3\n",
    "        else: # current_type == 3\n",
    "            if current_month == '12':\n",
    "                current_month = '07'\n",
    "                current_type = 1\n",
    "            else: # current_month == '07'\n",
    "                current_year -= 1\n",
    "                current_month = '12'\n",
    "                current_type = 1\n",
    "        \n",
    "        if current_year < end_year or (current_year == end_year and current_month == '07' and current_type == 3):\n",
    "            print(\"ğŸ ëª¨ë“  ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ì¡°ê±´ ì¶©ì¡±.\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        continue\n",
    "\n",
    "    # íŒŒì¼ ì´ë¦„ ìƒì„± ë° ì €ì¥ ë¡œì§ ë³€ê²½\n",
    "    if current_type == 3:\n",
    "        # G ìœ í˜• ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì €ì¥\n",
    "        if scraped_markdown_G.strip(): # ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°\n",
    "            file_name_G = f\"N{level_num}_{date_url}_G\"\n",
    "            save_to_markdown(file_name_G, scraped_markdown_G)\n",
    "        else:\n",
    "            print(f\"â„¹ï¸ {url} ì—ì„œ G ìœ í˜• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        # R ìœ í˜• ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì €ì¥\n",
    "        if scraped_markdown_R.strip(): # ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°\n",
    "            file_name_R = f\"N{level_num}_{date_url}_R\"\n",
    "            save_to_markdown(file_name_R, scraped_markdown_R)\n",
    "        else:\n",
    "            print(f\"â„¹ï¸ {url} ì—ì„œ G ìœ í˜• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    elif current_type == 1:\n",
    "        if scraped_markdown_V.strip(): # R ìœ í˜• ë°ì´í„° (scraped_markdown_Vì— ìˆìŒ)\n",
    "            file_name_V = f\"N{level_num}_{date_url}_V\"\n",
    "            save_to_markdown(file_name_V, scraped_markdown_V) # R ìœ í˜•ì€ V ë³€ìˆ˜ì— ë‹´ê²¨ ì˜´\n",
    "        else:\n",
    "            print(f\"â„¹ï¸ {url} ì—ì„œ R ìœ í˜• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # --- URL ì´ë™ ê·œì¹™ ì ìš© (ì´ì „ê³¼ ë™ì¼) ---\n",
    "    if current_type == 1:\n",
    "        current_type = 3\n",
    "    else: # current_type == 3\n",
    "        if current_month == '12':\n",
    "            current_month = '07'\n",
    "            current_type = 1\n",
    "        else: # current_month == '07'\n",
    "            current_year -= 1\n",
    "            current_month = '12'\n",
    "            current_type = 1\n",
    "    \n",
    "    # ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ ì¡°ê±´ í™•ì¸ (ì´ì „ê³¼ ë™ì¼)\n",
    "    if current_year < end_year or (current_year == end_year and current_month == '07' and current_type == 3):\n",
    "        print(\"ğŸ ëª¨ë“  ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ì¡°ê±´ ì¶©ì¡±.\")\n",
    "        break\n",
    "    \n",
    "    # ë´‡ ê°ì§€ ë°©ì§€ë¥¼ ìœ„í•œ ì„ì˜ì˜ ì§€ì—° ì‹œê°„ (ì´ì „ê³¼ ë™ì¼)\n",
    "    time.sleep(random.uniform(2, 5)) \n",
    "\n",
    "print(\"\\n--- ì›¹ ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
