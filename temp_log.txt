user@DESKTOP-G4U9APQ MINGW64 ~/Desktop/project/ai-service (enhance/32-problem-gen-enhance)
$ uvicorn app.main:app --reload
INFO:     Will watch for changes in these directories: ['C:\\Users\\user\\Desktop\\project\\ai-service']
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [24048] using StatReload
C:\Users\user\Desktop\project\ai-service\app\problem_generator\rag_chain.py:7: LangChainDeprecationWarning: Importing InMemoryCache from langchain is deprecated. Please replace deprecated imports:

>> from langchain import InMemoryCache

with new imports of:

>> from langchain_community.cache import InMemoryCache
You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>
  from langchain.cache import InMemoryCache
LangChain InMemoryCache가 활성화되었습니다.
'faiss_index'를 찾을 수 없습니다. 새로운 벡터 저장소를 생성합니다.
100%|███████████████████████████████████████████████████████████████████████████████████| 251/251 [00:23<00:00, 10.61it/s]
JLPT 데이터에서 251개의 청크를 생성했습니다.
100%|███████████████████████████████████████████████████████████████████████████████████| 302/302 [00:06<00:00, 48.59it/s]
Article 데이터에서 1202개의 청크를 생성했습니다.
총 1453개의 청크를 임베딩합니다.
1453개의 텍스트를 100개씩 일괄 처리하여 벡터 저장소를 생성합니다.
Process SpawnProcess-1:
Traceback (most recent call last):
  File "C:\Users\user\anaconda3\Lib\multiprocessing\process.py", line 314, in _bootstrap
    self.run()
  File "C:\Users\user\anaconda3\Lib\multiprocessing\process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\uvicorn\_subprocess.py", line 80, in subprocess_started
    target(sockets=sockets)
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\uvicorn\server.py", line 66, in run
    return asyncio.run(self.serve(sockets=sockets))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\anaconda3\Lib\asyncio\runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "C:\Users\user\anaconda3\Lib\asyncio\runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\anaconda3\Lib\asyncio\base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\uvicorn\server.py", line 70, in serve
    await self._serve(sockets)
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\uvicorn\server.py", line 77, in _serve
    config.load()
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\uvicorn\config.py", line 435, in load
    self.loaded_app = import_from_string(self.app)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\uvicorn\importer.py", line 19, in import_from_string
    module = importlib.import_module(module_str)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\anaconda3\Lib\importlib\__init__.py", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1387, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1360, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1331, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 995, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "C:\Users\user\Desktop\project\ai-service\app\main.py", line 4, in <module>
    from .problem_generator.router import router as problem_generator_router
  File "C:\Users\user\Desktop\project\ai-service\app\problem_generator\router.py", line 10, in <module>
    from .rag_chain import rag_chain
  File "C:\Users\user\Desktop\project\ai-service\app\problem_generator\rag_chain.py", line 86, in <module>
    vectorstore = FAISS.from_texts(texts[:batch_size], embeddings, metadatas=metadatas[:batch_size])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\langchain_community\vectorstores\faiss.py", line 1043, in from_texts
    embeddings = embedding.embed_documents(texts)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\langchain_openai\embeddings\base.py", line 590, in embed_documents
    return self._get_len_safe_embeddings(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\langchain_openai\embeddings\base.py", line 478, in _get_len_safe_embeddings
    response = self.client.create(
               ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\openai\resources\embeddings.py", line 129, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\openai\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\langchain-basic-Ybh8oqrH-py3.12\Lib\site-packages\openai\_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'Requested 547099 tokens, max 300000 tokens per request', 'type': 'max_tokens_per_request', 'param': None, 'code': 'max_tokens_per_request'}}
